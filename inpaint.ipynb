{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLimkmRcqOId"
      },
      "source": [
        "# Text-guided image-inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VvJ3IW0qOIe"
      },
      "source": [
        "The [StableDiffusionInpaintPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline) allows you to edit specific parts of an image by providing a mask and a text prompt. It uses a version of Stable Diffusion, like [`runwayml/stable-diffusion-inpainting`](https://huggingface.co/runwayml/stable-diffusion-inpainting) specifically trained for inpainting tasks.\n",
        "\n",
        "Get started by loading an instance of the [StableDiffusionInpaintPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wes_fEPAqOIf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\bryan\\.conda\\envs\\blended-diffusion\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bryan\\.cache\\huggingface\\hub\\models--kandinsky-community--kandinsky-2-2-decoder-inpaint. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "\nKandinskyV22InpaintCombinedPipeline requires the transformers library but it was not found in your environment. You can install it with pip: `pip\ninstall transformers`\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoPipelineForInpainting\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_image, make_image_grid\n\u001b[1;32m----> 5\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mAutoPipelineForInpainting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkandinsky-community/kandinsky-2-2-decoder-inpaint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m pipeline\u001b[38;5;241m.\u001b[39menable_model_cpu_offload()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\bryan\\.conda\\envs\\blended-diffusion\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\bryan\\.conda\\envs\\blended-diffusion\\lib\\site-packages\\diffusers\\pipelines\\auto_pipeline.py:1039\u001b[0m, in \u001b[0;36mAutoPipelineForInpainting.from_pretrained\u001b[1;34m(cls, pretrained_model_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1036\u001b[0m inpainting_cls \u001b[38;5;241m=\u001b[39m _get_task_class(AUTO_INPAINT_PIPELINES_MAPPING, orig_class_name)\n\u001b[0;32m   1038\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mload_config_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 1039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inpainting_cls\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\bryan\\.conda\\envs\\blended-diffusion\\lib\\site-packages\\diffusers\\utils\\dummy_torch_and_transformers_objects.py:947\u001b[0m, in \u001b[0;36mKandinskyV22InpaintCombinedPipeline.from_pretrained\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 947\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\bryan\\.conda\\envs\\blended-diffusion\\lib\\site-packages\\diffusers\\utils\\import_utils.py:670\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m    668\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersatileDiffusionTextToImagePipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersatileDiffusionPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnCLIPPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    678\u001b[0m ] \u001b[38;5;129;01mand\u001b[39;00m is_transformers_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.25.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    680\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install `transformers>=4.25` in order to use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m pip install\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m --upgrade transformers \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
            "\u001b[1;31mImportError\u001b[0m: \nKandinskyV22InpaintCombinedPipeline requires the transformers library but it was not found in your environment. You can install it with pip: `pip\ninstall transformers`\n"
          ]
        }
      ],
      "source": [
        "import PIL\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "\n",
        "pipeline = StableDiffusionInpaintPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-inpainting\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipeline = pipeline.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-z3V2NUqOIg"
      },
      "source": [
        "Download an image and a mask of a dog which you'll eventually replace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-kopc2bqOIg"
      },
      "outputs": [],
      "source": [
        "def download_image(url):\n",
        "    response = requests.get(url)\n",
        "    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "\n",
        "\n",
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
        "\n",
        "init_image = download_image(img_url).resize((512, 512))\n",
        "mask_image = download_image(mask_url).resize((512, 512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_2YA7s-qOIg"
      },
      "source": [
        "Now you can create a prompt to replace the mask with something else:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPWsXqX-qOIh"
      },
      "outputs": [],
      "source": [
        "prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n",
        "image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZv7zo0-qOIh"
      },
      "source": [
        "`image`          | `mask_image` | `prompt` | output |\n",
        ":-------------------------:|:-------------------------:|:-------------------------:|-------------------------:|\n",
        "<img src=\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" alt=\"drawing\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\" alt=\"drawing\" width=\"250\"/> | ***Face of a yellow cat, high resolution, sitting on a park bench*** | <img src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/in_paint/yellow_cat_sitting_on_a_park_bench.png\" alt=\"drawing\" width=\"250\"/> |\n",
        "\n",
        "\n",
        "<Tip warning={true}>\n",
        "\n",
        "A previous experimental implementation of inpainting used a different, lower-quality process. To ensure backwards compatibility, loading a pretrained pipeline that doesn't contain the new model will still apply the old inpainting method.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Check out the Spaces below to try out image inpainting yourself!\n",
        "\n",
        "<iframe\n",
        "\tsrc=\"https://runwayml-stable-diffusion-inpainting.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"500\"\n",
        "></iframe>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "blended-diffusion",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
